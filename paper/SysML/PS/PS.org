#+TITLE: Scaling Distributed Machine Learning With The Parameter Server
#+LATEX_HEADER: \usepackage{ctex}
#+LATEX_COMPILER: xelatex
- OSDI 2014

* Abstract
We propose a parameter server framework for distributed machine learning problems. 
* 1 Introduction
Sharing imposes three challenges:(带宽、同步、容错)
- Accessing the parameters requires an enormous amount of network bandwidth.
- Many machine learning algorithms are sequential. The resulting barriers hurt performance when the cost of synchronization and machine latency is high.
- At scale, fault tolerance is critical. Learning tasks are often performed in a cloud environment where machines can be unreliable and jobs can be preempted.

 [[file:PS.org_imgs/20201108_162123_doWREW.png]] 

 _对第三点的解释。Here, task failure is mostly due to being preempted or losing machines without necessary fault tolerance mechanisms._
** 1.1 Contributions
该文提出第三代的参数服务器。It confers two advantages to developers:
- enables application-specific code to remain concise
- _it provides a robust, versatile, and high-performance implementation capable of handling a diverse array of algorithms from sparse logistic regression to topic models and distributed sketching._

 Our parameter server provides five key features: 
  - Efficient communication
  - Flexible consistency models
  - Elastic Scalability
  - Fault Tolerance and Durability
  - Ease of Use
** 1.2 Engineering Challenges
reading and updating parameters shared between different worker nodes is ubiquitous. parameter server 保存部分参数，worker node 需要这些参数的子集。两个挑战：
- Communication
  parameter: key-value pairs, 值通常很小，每次跟新操作开销很大，采用这种方式低效。解决办法：参数通常表示为向量或张量等，
- Fault tolerance
  failover 故障转移，self-repair 能为动态扩展提供支持
  
  [[file:PS.org_imgs/20201109_162042_R64TTN.png]] 

  [[file:PS.org_imgs/20201109_162533_ZZHIl4.png]] 

  简而言之，我们的工作比公开的其他工作要好。
** 1.3 Related Work
发展历程：
- The first generation of such parameter servers: memcached distributed(key, value) store
- The second generation: Distbelief
- The third generation: Ours

其他：
- GraphLab 缺乏弹性的可扩展性
- Piccolo 使用参数服务器的策略进行多台机器的状态共享和聚合，他因此实现了我们系统的大部分功能，但缺少优化策略（message compression, replication, and variable consistency models expressed via dependency graphs）
* 2 Machine Learning
** 2.1 Goals
目标函数(objective function), 如何快速训练大量的数据是我们的关注的重点。
** 2.2 Risk Minimization
rish -- prediction error, overfit, 正则化, loss, 一些机器学习的概念等等。损失函数和正则化对本文不重要。

_我们简单描述为distributed subgradient descent: 次梯度方面只是简单地推广到损失函数和不需要连续可微的正则化器，如在w = 0的|w|_

[[file:PS.org_imgs/20201109_205205_0B2DJo.png]]

每个worker只训练自己的数据，得到subgradient，然后聚集跟新w。

[[file:PS.org_imgs/20201109_205231_utqe0Y.png]]

时间开销最大的是计算subgradient，但是计算被分配到多台机器。

[[file:PS.org_imgs/20201110_140316_aTJmHM.png]]

worker越多，每个worker上的参数就越少。 _参数不应该是固定数量的吗？网络是全连接的？_
** 2.3 Generative Models
对于无监督学习，算法1需要进行改进，每次跟新不是梯度，而是与文档主题的契合度（topic modeling），同时需要补充信息（单词的含义）。
* 3 Architecture
[[file:PS.org_imgs/20201110_182452_3VgbEb.png]]

每个参数服务器既支持单独的参数空间，又支持共享参数空间。
** 3.1 (Key, Value) vectors
the pair is a feature ID and its weight.
** 3.2 Range Push and Pull
w.push(R, dest), w.pull(R, dest) 为了编程和效率，支持range-based push and pull
** 3.3 User-Defined Functions on the Server
服务器可以执行用户定义的函数
** 3.4 Asynchronous Tasks and Dependency
- 任务是同步的：
  + The caller marks a task as finished only once it receives the callee’s reply.
  + The callee marks a task as finished only if the call of the task is returned and all subtasks issued by this call are finished
- 任务以来能提高算法逻辑性和模型的一致性
[[file:PS.org_imgs/20201110_184541_yCmmwR.png]] 

** 3.5 Flexible Consistency
任务的独立性能提高并行程度，提高系统效率，但是导致不一致性，因此需要trade-off。

关于任务之间的以来可以建立三种模型：Sequential, Eventual, Bounded Delay。值得注意的是，这些图应该是动态的，因为时间是可变的。

[[file:PS.org_imgs/20201110_184713_Wz0I8x.png]]
** 3.6 User-defined Filters
使用用户定义的过滤器选择需要同步的key-value pairs。KKT: a worker only pushes gradients that are likely to affect the weights on the servers
* 4 Implementation
** 4.1 Vector Clock
- 用来记录每个节点(key, value) pair的时间。
- 许多参数时间戳相同
[[file:PS.org_imgs/20201110_185915_tUCyTP.png]]

[[file:PS.org_imgs/20201110_214651_eJdEsX.png]]
** 4.2 Messages
[[file:PS.org_imgs/20201110_190122_KcTtuT.png]]
参数服务器的基本通信格式，我们使用fast Snappy compression library压缩信息。
** 4.3 Consistent Hashing
[[file:PS.org_imgs/20201110_190423_IeaQJ7.png]]
每个服务器节点负责它的开始到下一个节点，该节点是该key range的master。
** 4.4 Replication and Consistency
[[file:PS.org_imgs/20201110_191114_nNFZVT.png]]
过多的复制会影响带宽。
** 4.5 Server Management
为了提高系统容错和动态扩展能力，我们必须支持添加和删除节点。
** 4.6 Worker Management
添加和删除worker node
* 5 Evaluation
Sparse Logistic Regression and Latent Dirichlet Allocation
** 5.1 Sparse Logistic Regression
- Problem and Data: ad click prediction dataset, 1000 machines
- Algorithm:
  [[file:PS.org_imgs/20201110_214356_4k00Hx.png]] 
  它与之前的有四点不同：
  + 每次迭代只有一部分参数被更新
  + workers 计算梯度和这一部分参数的二阶导数
  + 服务器根据聚集的局部梯度求解近似算子来更新模型
  + bounded-delay model, Karush-Kuhn-Tucker (KKT) filter
- Results:
  compare ours with two others, 一些比较结果
** 5.2 Latent Dirichlet Allocation
用户兴趣，LDA
** 5.3 Sketches
...
* 6 Summary and Discussion
- 提出PS架构
- 易用：全局参数共享。高效：所有通信都是同步的。灵活的一致性：在效率和算法的收敛速度折中。
- 弹性扩展和容错
