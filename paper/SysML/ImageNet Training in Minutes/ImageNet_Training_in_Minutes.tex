% Created 2020-11-12 四 20:53
% Intended LaTeX compiler: xelatex
\documentclass[11pt]{article}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{ctex}
\author{Wang Jian}
\date{\today}
\title{ImageNet training in minutes}
\hypersetup{
 pdfauthor={Wang Jian},
 pdftitle={ImageNet training in minutes},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 27.1 (Org mode 9.4)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents

\section{Background}
\label{sec:orgdf0aafd}
\begin{itemize}
\item 如果我们能够重复利用计算机的算力，那么训练能够在几秒钟完成
\item 在分布式系统中，增大 batch size k 倍, 每个 epoch 迭代次数缩小 k 倍，训练越快，通信开销降低 k 倍，但是可能带来准确率的降低。如何增大 batch size? warmup, LARS
\item PS模型
\item Model Parallelism vs Data Parallelism
\end{itemize}

\section{Main Work}
\label{sec:org6065034}
\subsection{Synchronous SGD}
\label{sec:orgbc66159}
\begin{itemize}
\item Synchronous SGD 主要优点：sequential consistency
\end{itemize}
\subsection{warm-up scheme}
\label{sec:org660ccce}
\begin{itemize}
\item Linear Scaling: 增大 batch size k 倍，学习率应该增大 k 倍
\item 从较小的学习率开始逐渐增加
\end{itemize}
\subsection{Layer-wise Adaptive Rate Scaling(LARS)}
\label{sec:orge4a057a}
\begin{itemize}
\item 在使用 Synchronous SGD 时，通过 LARS 帮助选择学习率
\end{itemize}
\subsection{Summary}
\label{sec:orgc6353e6}
\begin{itemize}
\item 使用 LARS 和 warmup scheme 算法来扩展 batch size
\item Using LARS we efficiently utilized 1024 CPUs to finish the 100-epoch ImageNet training with AlexNet in 11 minutes with 58.6\% accuracy (batch size = 32K),
\item We got 74.9\% top-1 test accuracy in 64 epochs, which only needs 14 minutes.
\end{itemize}
\section{Innovation}
\label{sec:org422c83f}
\begin{itemize}
\item 使用 LARS 和 warmup scheme 算法来扩展 batch size
\end{itemize}
\section{comment}
\label{sec:org7cc3669}
本文从batch size 对分布式系统的影响入手，提出使用 LARS 和 warmup scheme 算法来扩展 batch size. 重点开始需要有设备才能做出来。
\end{document}
