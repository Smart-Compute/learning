{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AAlDtDUt1QP5"
   },
   "source": [
    "# Handwritten Digit Recognition\n",
    "\n",
    "**该教程需要的mxnet版本为1.7左右，2.0.0不行**\n",
    "\n",
    "This tutorial guides you through implementing a classic computer vision application: identifying handwritten digits with neural networks. \n",
    "    \n",
    "## Learning points\n",
    "1. How to prepare image data for training?\n",
    "    * batching and color channels\n",
    "    * 2d or 4d tensors for raw image data\n",
    "2. How to use a high-level framework (MXNet/Gluon) to create a neural model?\n",
    "    * fully-connected and convolution layers\n",
    "    * stacking layers\n",
    "3. How to train the model?\n",
    "    * typical training loop\n",
    "    * model evaluation\n",
    "    \n",
    "## Note about neural nets architecture and MXNet API\n",
    "\n",
    "A good source to learn more about why we design neural nets the way we do, and the rationale behind various layers is \n",
    "[Stanford cs231n class](https://cs231n.github.io/).\n",
    "\n",
    "To learn more about MXNet Gluon API, check out\n",
    "[Deep Learning --- The Straight Dope](http://gluon.mxnet.io/index.html)\n",
    "\n",
    "## Prepare data\n",
    "\n",
    "We first download the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset, a commonly used dataset for handwritten digit recognition.\n",
    "\n",
    "* Each image in this dataset has been center-cropped to 28x28 with grayscale pixel value [0, 255].\n",
    "* Each image has only a single color channel, rather than three channels as for RGB images.\n",
    "* There are two separate sets of files. The training set contains 60000 examples, and the test set 10000 examples.\n",
    "* Each example includes an image and a label.\n",
    "\n",
    "See more details about the dataset at http://yann.lecun.com/exdb/mnist/.\n",
    "\n",
    "### 2d and 4d tensors for raw images\n",
    "\n",
    "You are familiar with images being stored as a 2d matrix, (width, height). For an image with multiple color channels, you can use a 3d tensor (color_channel, width, height).\n",
    "\n",
    "If we have a bunch of images, we can use 4d tensors to represent them, by adding a new batch_size dimension,\n",
    "\n",
    "**(batch_size, color_channel, width, height)**\n",
    "\n",
    "For example, the grayscale digit image (28, 28) can be stored in memory as (1, 28, 28), and a batch of 100 images can be stored as a **(100, 1, 28, 28)** tensor. \n",
    "    \n",
    "This 4d tensor format for raw images would be later used as input to convolution layer.\n",
    "\n",
    "An alternative way to represent the raw images is to flatten all the dimensions except for the batch_size dimension.\n",
    "\n",
    "For example, (100, 1, 28, 28) can be flattened to (100, 784). This 2d tensor format is later used as input to fully-connected layer.\n",
    "\n",
    "Now, let's first download and load the images and the corresponding labels. Then, we will process the raw images to the appropriate formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 71,
     "output_extras": [
      {
       "item_id": 2
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1272,
     "status": "ok",
     "timestamp": 1522214706090,
     "user": {
      "displayName": "Qiao Zhang",
      "photoUrl": "//lh4.googleusercontent.com/-SXndlxxq5jE/AAAAAAAAAAI/AAAAAAAAErc/EMDhiQCYheU/s50-c-k-no/photo.jpg",
      "userId": "111921404224074339099"
     },
     "user_tz": 420
    },
    "id": "P0cch9SczvXb",
    "outputId": "631582d9-7797-47e4-cb9c-b1da74d33494"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-labels-idx1-ubyte.gz\n",
      "train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scrutiny/.local/lib/python3.6/site-packages/ipykernel_launcher.py:20: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t10k-labels-idx1-ubyte.gz\n",
      "t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scrutiny/.local/lib/python3.6/site-packages/ipykernel_launcher.py:23: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import urllib.request\n",
    "import gzip\n",
    "import struct\n",
    "\n",
    "def download_data(url, force_download=True):\n",
    "    \"\"\"Download data file to disk and returns filename.\"\"\"\n",
    "    fname = url.split(\"/\")[-1]\n",
    "    if force_download or not os.path.exists(fname):\n",
    "        print(fname)\n",
    "#         urllib.request.urlretrieve(url, fname)\n",
    "    return fname\n",
    "\n",
    "def read_data(label_url, image_url):\n",
    "    \"\"\"Download and deserialize raw data to numpy ndarray. Return (label, image) tuple.\"\"\"\n",
    "    # the original files are gzip-compressed with a particular serialization format\n",
    "    with gzip.open(download_data(label_url)) as flbl:\n",
    "        magic, num = struct.unpack(\">II\", flbl.read(8))\n",
    "        label = np.fromstring(flbl.read(), dtype=np.int8)\n",
    "    with gzip.open(download_data(image_url), 'rb') as fimg:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "        image = np.fromstring(fimg.read(), dtype=np.uint8).reshape(len(label), rows, cols)\n",
    "    return (label, image)\n",
    "\n",
    "path='http://yann.lecun.com/exdb/mnist/'\n",
    "\n",
    "# label, image tuple\n",
    "(train_lbl, train_img) = read_data(\n",
    "    path + 'train-labels-idx1-ubyte.gz', path + 'train-images-idx3-ubyte.gz')\n",
    "(test_lbl, test_img) = read_data(\n",
    "    path + 't10k-labels-idx1-ubyte.gz', path + 't10k-images-idx3-ubyte.gz')\n",
    "\n",
    "# check raw data\n",
    "\n",
    "# type(train_lbl)\n",
    "# numpy.ndarray\n",
    "# https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.ndarray.html\n",
    "\n",
    "# train_lbl.shape\n",
    "# (60000,)\n",
    "# train_lbl.dtype\n",
    "# dtype('int8')\n",
    "\n",
    "# train_img.shape\n",
    "# (60000, 28, 28)\n",
    "# train_img.dtype\n",
    "# dtype('uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_tNBPhLJ1NVg"
   },
   "source": [
    "To get a sense for the raw digit images, we can plot the first 10 images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 97,
     "output_extras": [
      {
       "item_id": 1
      },
      {
       "item_id": 2
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 898,
     "status": "ok",
     "timestamp": 1522214709396,
     "user": {
      "displayName": "Qiao Zhang",
      "photoUrl": "//lh4.googleusercontent.com/-SXndlxxq5jE/AAAAAAAAAAI/AAAAAAAAErc/EMDhiQCYheU/s50-c-k-no/photo.jpg",
      "userId": "111921404224074339099"
     },
     "user_tz": 420
    },
    "id": "oLtou7s-z8Qc",
    "outputId": "7c11cc2c-9b3e-4693-96c1-cb95e03faf90"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: [5 0 4 1 9 2 1 3 1 4]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_inst = 10\n",
    "for i in range(num_inst):\n",
    "    plt.subplot(1,num_inst,i+1)\n",
    "    # train_img[i] has shape (28, 28)\n",
    "    plt.imshow(train_img[i], cmap='Greys_r')\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "print('label: %s' % (train_lbl[0:num_inst],))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bb8PIbh21oQI"
   },
   "source": [
    "### Data iterator to iterate raw data by mini-batches\n",
    "\n",
    "We can train neural network by looking at one image/label at a time, but when there is parallelism from the CPU/GPU hardware,it is far more efficient to look at batches of images at a time.\n",
    "\n",
    "To iterate images by batches, we can manually write a for loop and partition the data into chunks and look at one chunk at a time. A simpler way is to wrap the raw data using data iterators typically already provided from deep learning frameworks. \n",
    "\n",
    "Let's use mx.gluon.data.DataLoader and gluon.data.ArrayDataset to create our data iterators.\n",
    "\n",
    "Note that there's an option to shuffle the data rows, which is typically done for training data, but not validation/test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 88,
     "output_extras": [
      {
       "item_id": 3
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 48944,
     "status": "ok",
     "timestamp": 1522214769021,
     "user": {
      "displayName": "Qiao Zhang",
      "photoUrl": "//lh4.googleusercontent.com/-SXndlxxq5jE/AAAAAAAAAAI/AAAAAAAAErc/EMDhiQCYheU/s50-c-k-no/photo.jpg",
      "userId": "111921404224074339099"
     },
     "user_tz": 420
    },
    "id": "Q3VLrgKqzSV_",
    "outputId": "7f73ff1c-96ec-4946-ed89-f69dbf106b85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 1466105856 bytes == 0x5609ed6dc000 @  0x7f734cd83107 0x5609911664ed 0x5609911e9ac0 0x56099120ed29 0x56099116d90e 0x560991232021 0x56099118477a 0x560991189462 0x560991181b3a 0x56099118982e 0x560991181b3a 0x56099119d809 0x5609911b656e 0x56099116d90e 0x5609911feb76 0x560991189650 0x560991189462 0x560991181b3a 0x56099118982e 0x560991181b3a 0x56099118982e 0x56099119cd68 0x560991184c3f 0x56099119cd68 0x560991184c3f 0x56099119cd68 0x560991184c3f 0x560991181b3a 0x560991189e1f 0x560991181b3a 0x560991189e1f\n",
      "tcmalloc: large alloc 1466105856 bytes == 0x560a44d0c000 @  0x7f734cd83107 0x5609911664ed 0x5609911e9ac0 0x5609911c840e 0x5609911f7577 0x56099116d90e 0x56099118dad0 0x560991106334 0x560991217be5 0x56099118477a 0x560991181b3a 0x56099118982e 0x560991181b3a 0x56099119d809 0x5609911b656e 0x56099116d90e 0x5609911feb76 0x560991189650 0x560991189462 0x560991181b3a 0x56099118982e 0x560991181b3a 0x56099118982e 0x56099119cd68 0x560991184c3f 0x56099119cd68 0x560991184c3f 0x56099119cd68 0x560991184c3f 0x560991181b3a 0x560991189e1f\n",
      "tcmalloc: large alloc 1466105856 bytes == 0x560a9c33c000 @  0x7f734cd83107 0x560991168062 0x5609911cf32e 0x5609911f759a 0x56099116d90e 0x56099118dad0 0x560991106334 0x560991217be5 0x56099118477a 0x560991181b3a 0x56099118982e 0x560991181b3a 0x56099119d809 0x5609911b656e 0x56099116d90e 0x5609911feb76 0x560991189650 0x560991189462 0x560991181b3a 0x56099118982e 0x560991181b3a 0x56099118982e 0x56099119cd68 0x560991184c3f 0x56099119cd68 0x560991184c3f 0x56099119cd68 0x560991184c3f 0x560991181b3a 0x560991189e1f 0x560991181b3a\n"
     ]
    }
   ],
   "source": [
    "# !pip install -q mxnet-cu80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KkZqyeniEb2T"
   },
   "source": [
    "In Colab Runtime menu tab, select \"Change runtime type\" and select GPU for Hardware accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 88,
     "output_extras": [
      {
       "item_id": 3
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 47954,
     "status": "ok",
     "timestamp": 1522214215178,
     "user": {
      "displayName": "Qiao Zhang",
      "photoUrl": "//lh4.googleusercontent.com/-SXndlxxq5jE/AAAAAAAAAAI/AAAAAAAAErc/EMDhiQCYheU/s50-c-k-no/photo.jpg",
      "userId": "111921404224074339099"
     },
     "user_tz": 420
    },
    "id": "n67FJo6CCII7",
    "outputId": "ad18c95f-5562-4662-badb-0cdfb9f32a7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 1534713856 bytes == 0x55b5c087c000 @  0x7f3334347107 0x55b558d174ed 0x55b558d9aac0 0x55b558dbfd29 0x55b558d1e90e 0x55b558de3021 0x55b558d3577a 0x55b558d3a462 0x55b558d32b3a 0x55b558d3a82e 0x55b558d32b3a 0x55b558d4e809 0x55b558d6756e 0x55b558d1e90e 0x55b558dafb76 0x55b558d3a650 0x55b558d3a462 0x55b558d32b3a 0x55b558d3a82e 0x55b558d32b3a 0x55b558d3a82e 0x55b558d4dd68 0x55b558d35c3f 0x55b558d4dd68 0x55b558d35c3f 0x55b558d4dd68 0x55b558d35c3f 0x55b558d32b3a 0x55b558d3ae1f 0x55b558d32b3a 0x55b558d3ae1f\n",
      "tcmalloc: large alloc 1534713856 bytes == 0x55b61c01a000 @  0x7f3334347107 0x55b558d174ed 0x55b558d9aac0 0x55b558d7940e 0x55b558da8577 0x55b558d1e90e 0x55b558d3ead0 0x55b558cb7334 0x55b558dc8be5 0x55b558d3577a 0x55b558d32b3a 0x55b558d3a82e 0x55b558d32b3a 0x55b558d4e809 0x55b558d6756e 0x55b558d1e90e 0x55b558dafb76 0x55b558d3a650 0x55b558d3a462 0x55b558d32b3a 0x55b558d3a82e 0x55b558d32b3a 0x55b558d3a82e 0x55b558d4dd68 0x55b558d35c3f 0x55b558d4dd68 0x55b558d35c3f 0x55b558d4dd68 0x55b558d35c3f 0x55b558d32b3a 0x55b558d3ae1f\n",
      "tcmalloc: large alloc 1534713856 bytes == 0x55b6777b8000 @  0x7f3334347107 0x55b558d19062 0x55b558d8032e 0x55b558da859a 0x55b558d1e90e 0x55b558d3ead0 0x55b558cb7334 0x55b558dc8be5 0x55b558d3577a 0x55b558d32b3a 0x55b558d3a82e 0x55b558d32b3a 0x55b558d4e809 0x55b558d6756e 0x55b558d1e90e 0x55b558dafb76 0x55b558d3a650 0x55b558d3a462 0x55b558d32b3a 0x55b558d3a82e 0x55b558d32b3a 0x55b558d3a82e 0x55b558d4dd68 0x55b558d35c3f 0x55b558d4dd68 0x55b558d35c3f 0x55b558d4dd68 0x55b558d35c3f 0x55b558d32b3a 0x55b558d3ae1f 0x55b558d32b3a\n"
     ]
    }
   ],
   "source": [
    "#!pip install -q mxnet-cu80==1.2.0b20180327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "y4046CtUz_oN"
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon\n",
    "mx.random.seed(1)\n",
    "\n",
    "def to4d(img):\n",
    "    \"\"\"Reshape img to 4d tensor and normalize pixel values to [0, 1].\"\"\"\n",
    "    return img.reshape(img.shape[0], 1, 28, 28).astype(np.float32)/255\n",
    "\n",
    "batch_size = 100\n",
    "train_iter = mx.gluon.data.DataLoader(gluon.data.ArrayDataset(to4d(train_img), train_lbl), batch_size, shuffle=True)\n",
    "test_iter = mx.gluon.data.DataLoader(gluon.data.ArrayDataset(to4d(test_img), test_lbl), batch_size)\n",
    "\n",
    "# DataLoader has a method\n",
    "# def __iter__(self):\n",
    "# to see how iterators work, try\n",
    "# for i, (data, label) in enumerate(train_iter):\n",
    "#     print i, data.shape, label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zoRf6H371vsf"
   },
   "source": [
    "## Multilayer Perceptron (MLP)\n",
    "\n",
    "Now, we are ready to create neural models.\n",
    "\n",
    "One simple model is called multilayer perceptron (MLP).\n",
    "MLP is made up of several fully-connected layers (or called dense layers) stacked one after another, followed by a softmax layer for label prediction.\n",
    "\n",
    "A fully-connected layer is an affine transformation taking input tensor *X* to output tensor *Y*, with two parameters, weight *W* and bias *b*.\n",
    "\n",
    "For the non-batched version, we have the following dimensions for each variable,\n",
    "\n",
    "\\begin{align}\n",
    "\\underset{1xk}{Y} &= \\underset{1xm}{x} \\underset{mxk}{W} + \\underset{1xk}{b}\n",
    "\\end{align}\n",
    "\n",
    "We usually work with batched version, and it has the following dimensions for each variable,\n",
    "\n",
    "\\begin{align}\n",
    "\\underset{nxk}{Y} &= \\underset{nxm}{X} \\underset{mxk}{W} + \\underset{nxk}{B}\n",
    "\\end{align}\n",
    "\n",
    "Note that the bias vector is usually specified as a vector 1xk, rather than a full tensor nxk, and relies on array broadcasting to get the right dimension.\n",
    "\n",
    "The output of a hidden layer is then passed through a non-linear activation layer, e.g., apply sigmoid, tanh or ReLU on output element-wise.\n",
    "\n",
    "The output of the activation layer can be fed as input to the next fully-connected layer. In this way, we can chain multiple full-connected layer.\n",
    "\n",
    "Note that if we do not use activation layer, multiple affine transformation is equivalent to a single affine transformation. Activation layer is necessary to add non-linearity.\n",
    "\n",
    "The last fully-connected layer often has the hidden size equals to the number of classes in the dataset. \n",
    "Then we add a softmax layer, which map the input into a probability score.\n",
    "Assume the input *X* has size *n x m*, the output *Y* would also have size *n x m*, where each row is a normalized probability vector,\n",
    "\n",
    "$$ \\left[\\frac{\\exp(x_{i1})}{\\sum_{j=1}^m \\exp(x_{ij})},\\ldots, \\frac{\\exp(x_{im})}{\\sum_{j=1}^m \\exp(x_{ij})}\\right] $$\n",
    "\n",
    "To evaluate how well the predictions are, we use cross-entropy loss,\n",
    "\n",
    "\\begin{align}\n",
    "L = \\frac{1}{N} \\sum y \\log{\\hat{y}}\n",
    "\\end{align}\n",
    "where *y* is the one-hot vector of true label, and $\\hat{y}$ is the estimated probability from softmax layer.\n",
    "\n",
    "Here's how we would create a multilayer perceptron in MXNet using Gluon frontend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 102,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 330,
     "status": "ok",
     "timestamp": 1522214801219,
     "user": {
      "displayName": "Qiao Zhang",
      "photoUrl": "//lh4.googleusercontent.com/-SXndlxxq5jE/AAAAAAAAAAI/AAAAAAAAErc/EMDhiQCYheU/s50-c-k-no/photo.jpg",
      "userId": "111921404224074339099"
     },
     "user_tz": 420
    },
    "id": "ICKfWyh00BB-",
    "outputId": "6df617fb-226a-45fa-9289-e15209d1b266"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'name_scope'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2b4ccd7217c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'name_scope'"
     ]
    }
   ],
   "source": [
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "num_examples = 60000\n",
    "\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Dense(128, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(64, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_outputs))\n",
    "\n",
    "# at the end of the 3rd dense layer, there is no activation layer after it\n",
    "# you can use a softmax layer to make predictions or use a softmax_cross_entropy layer to evaluate loss function\n",
    "    \n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "# visualize the network\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uPAEf0YI17te"
   },
   "source": [
    "### Initialize parameters\n",
    "\n",
    "There are many ways to initialize parameters. For different types of layers, we may need different initialization functions.\n",
    "\n",
    "Read more about initialization at http://cs231n.github.io/neural-networks-2/#init."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Lt2p63Yu0Rtf"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'initialize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7f155ae5a132>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'initialize'"
     ]
    }
   ],
   "source": [
    "# model_ctx = mx.cpu()\n",
    "model_ctx = mx.gpu()\n",
    "\n",
    "net.collect_params().initialize(mx.init.Normal(sigma=.1), ctx=model_ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vQ-T9Slu2DnR"
   },
   "source": [
    "### Evaluation metric\n",
    "We can evaluate training/validation set loss by running predictions and compute the average prediction accuracy.\n",
    "\n",
    "Note that we can extract the predicted label from probability vector with argmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "UKjwQQmT0S5v"
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    \"\"\"Make predictions for the dataset and evaluate average accuracy.\"\"\"\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        # reshape to (batch_size, 784)\n",
    "        data = data.as_in_context(model_ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        output = net(data)\n",
    "        # skipped a softmax layer since it doesn't change predictions\n",
    "        # use argmax to extract predicted label\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oURJ-ArE2GOX"
   },
   "source": [
    "### Training Loop\n",
    "Now that both the network definition and data iterators are ready. We can start training. \n",
    "\n",
    "We make multiple complete passes over the training data.\n",
    "Each pass is called an epoch.\n",
    "In each epoch, we partition the training data to mini-batches.\n",
    "For each mini-batch, we compute a forward pass of the network to get the predicted output.\n",
    "Then, we do a backward pass, where we compute the loss function and the gradients of each model parameter.\n",
    "We then update each model parameter **w** with its gradient multipled by learning rate,\n",
    "\n",
    "\\begin{align}\n",
    "w = w - \\alpha \\nabla{w}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 187,
     "output_extras": [
      {
       "item_id": 10
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 271729,
     "status": "ok",
     "timestamp": 1522215082481,
     "user": {
      "displayName": "Qiao Zhang",
      "photoUrl": "//lh4.googleusercontent.com/-SXndlxxq5jE/AAAAAAAAAAI/AAAAAAAAErc/EMDhiQCYheU/s50-c-k-no/photo.jpg",
      "userId": "111921404224074339099"
     },
     "user_tz": 420
    },
    "id": "xqTbDHlM0UCP",
    "outputId": "a945e902-0a41-49f6-eba7-4dd09f3a0855"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 1.2759023540496826, Train_acc 0.8311333333333333, Test_acc 0.8337\n",
      "Epoch 1. Loss: 0.5154037885665893, Train_acc 0.8732833333333333, Test_acc 0.8808\n",
      "Epoch 2. Loss: 0.4042650778134664, Train_acc 0.89315, Test_acc 0.899\n",
      "Epoch 3. Loss: 0.3566678596496582, Train_acc 0.90425, Test_acc 0.9086\n",
      "Epoch 4. Loss: 0.32646166838010154, Train_acc 0.9108833333333334, Test_acc 0.9159\n",
      "Epoch 5. Loss: 0.30364461968739825, Train_acc 0.9165, Test_acc 0.9206\n",
      "Epoch 6. Loss: 0.2859980047702789, Train_acc 0.9193833333333333, Test_acc 0.9248\n",
      "Epoch 7. Loss: 0.27087084472974143, Train_acc 0.92405, Test_acc 0.9279\n",
      "Epoch 8. Loss: 0.2576552839120229, Train_acc 0.9278166666666666, Test_acc 0.9306\n",
      "Epoch 9. Loss: 0.24573933501243592, Train_acc 0.9301166666666667, Test_acc 0.9317\n"
     ]
    }
   ],
   "source": [
    "# get a optimizer doing SGD with fixed learning rate\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': .01})\n",
    "\n",
    "epochs = 10\n",
    "smoothing_constant = .01\n",
    "num_examples = 60000\n",
    "\n",
    "for e in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    for i, (data, label) in enumerate(train_iter):\n",
    "        # reshape to (batch_size, 784)\n",
    "        data = data.as_in_context(model_ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        # this \"with\" record allows us to do automatic differentiation\n",
    "        # basically remembering how the output is compute from the input\n",
    "        # we can use chain rules to find out the gradients of loss function wrt inputs\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        # compute all gradients\n",
    "        loss.backward()\n",
    "        # Perform the weight update.\n",
    "        # Trainer needs to know the batch size of data to normalize the gradients by 1/batch_size.\n",
    "        trainer.step(data.shape[0])\n",
    "        cumulative_loss += nd.sum(loss).asscalar()\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_iter, net)\n",
    "    train_accuracy = evaluate_accuracy(train_iter, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" %\n",
    "          (e, cumulative_loss/num_examples, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RHesfE542Kef"
   },
   "source": [
    "### Inference.\n",
    "\n",
    "After training is done, we can make prediction on test images. \n",
    "\n",
    "Let's try it for a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 282,
     "output_extras": [
      {
       "item_id": 1
      },
      {
       "item_id": 2
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 408,
     "status": "ok",
     "timestamp": 1522211790879,
     "user": {
      "displayName": "Qiao Zhang",
      "photoUrl": "//lh4.googleusercontent.com/-SXndlxxq5jE/AAAAAAAAAAI/AAAAAAAAErc/EMDhiQCYheU/s50-c-k-no/photo.jpg",
      "userId": "111921404224074339099"
     },
     "user_tz": 420
    },
    "id": "Z-xUJ6pk0VJP",
    "outputId": "db84550a-0bc2-4343-da38-8c7930b066ba"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted digit is 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scrutiny/.local/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# show test image\n",
    "plt.imshow(test_img[0], cmap='Greys_r')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# make prediction，using gpu\n",
    "output = net(nd.array(to4d(test_img[0:1]).reshape((-1, 784)), ctx=mx.gpu()))\n",
    "print(\"Predicted digit is\", np.asscalar(nd.argmax(output, axis=1).asnumpy().astype(np.int8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K6JCNZlf2PVq"
   },
   "source": [
    "## Convolutional Neural Networks (ConvNets)\n",
    "\n",
    "ConvNets is a neural architecture that has a reduced model parameter size compared to MLP, and has been shown to achieve higher accuracy on many vision datasets.\n",
    "\n",
    "ConvNets introduces two additional layers, the convolution layer and max-pooling layers, on top of fully-connected layers.\n",
    "\n",
    "Note that the previous fully-connected layer simply flattens the image to a vector, with input dimension (batch_size, 784).\n",
    "It ignores the spatial locality information for pixels near each other.\n",
    "\n",
    "The newly introduced convolutional layer aims to leverage the spatial locality by doing a convolution operation on the input image. \n",
    "The convolution operation can be visualized as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i07pzdO32QsW"
   },
   "source": [
    "<img src=\"https://thatindiandude.github.io/images/conv.png\" style=\"height: 75%; width: 75%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vpu4T5NI2WV9"
   },
   "source": [
    "The convolution is specified by kernel width, kernel height, num_channel, stride and padding.\n",
    "Note that the num_channel has to be the same as the num_channel dimension of the input image.\n",
    "\n",
    "We can also have multiple feature maps, each with their own weight matrices, to capture different features: \n",
    "<img src=\"https://thatindiandude.github.io/images/filters.png\" style=\"height: 75%; width: 75%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2MAyl0Sr2dH5"
   },
   "source": [
    "Besides the convolutional layer, another major innovation of ConvNets is the addition of pooling layers.\n",
    "A pooling layer reduce a $n\\times m$ (often called kernal size) image patch into a single value to make the network less sensitive to the spatial location.\n",
    "\n",
    "<img src=\"https://thatindiandude.github.io/images/pooling.png\" style=\"height: 75%; width: 75%;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 170,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 320,
     "status": "ok",
     "timestamp": 1522214039009,
     "user": {
      "displayName": "Qiao Zhang",
      "photoUrl": "//lh4.googleusercontent.com/-SXndlxxq5jE/AAAAAAAAAAI/AAAAAAAAErc/EMDhiQCYheU/s50-c-k-no/photo.jpg",
      "userId": "111921404224074339099"
     },
     "user_tz": 420
    },
    "id": "GWF1LTcL0rWD",
    "outputId": "45c85003-2e8f-4c31-e516-747ee230ed95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2D(None -> 20, kernel_size=(5, 5), stride=(1, 1), Activation(relu))\n",
      "  (1): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False, global_pool=False, pool_type=max, layout=NCHW)\n",
      "  (2): Conv2D(None -> 50, kernel_size=(5, 5), stride=(1, 1), Activation(relu))\n",
      "  (3): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False, global_pool=False, pool_type=max, layout=NCHW)\n",
      "  (4): Flatten\n",
      "  (5): Dense(None -> 512, Activation(relu))\n",
      "  (6): Dense(None -> 10, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_fc = 512\n",
    "num_outputs = 10\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Conv2D(channels=20, kernel_size=5, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))\n",
    "    net.add(gluon.nn.Conv2D(channels=50, kernel_size=5, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))\n",
    "    # The Flatten layer collapses all axis, except the first one, into one axis.\n",
    "    net.add(gluon.nn.Flatten())\n",
    "    net.add(gluon.nn.Dense(num_fc, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_outputs))\n",
    "\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    \n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wSPOTzF12hHp"
   },
   "source": [
    "Note that LeNet is more complex than the previous multilayer perceptron, so we use GPU instead of CPU for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "BypKXLkz6GoL"
   },
   "outputs": [],
   "source": [
    "model_ctx = mx.gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GXnkcxVz5kuX"
   },
   "source": [
    "Don't forget to initialize the model again. This time is a different initialization technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "zZe_P8tJ5q9F"
   },
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=model_ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ftDIurs29i9L"
   },
   "source": [
    "The usual training loop. Note that the raw image data is now fed as a 4d tensor to conv layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 187,
     "output_extras": [
      {
       "item_id": 10
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 692030,
     "status": "ok",
     "timestamp": 1522213960410,
     "user": {
      "displayName": "Qiao Zhang",
      "photoUrl": "//lh4.googleusercontent.com/-SXndlxxq5jE/AAAAAAAAAAI/AAAAAAAAErc/EMDhiQCYheU/s50-c-k-no/photo.jpg",
      "userId": "111921404224074339099"
     },
     "user_tz": 420
    },
    "id": "mDws-lZ_0sop",
    "outputId": "2cb7c03d-d408-42cc-bd57-6c33898c8364"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 1.1192459464708964, Train_acc 0.9034, Test_acc 0.9074\n",
      "Epoch 1. Loss: 0.27250371119181316, Train_acc 0.93895, Test_acc 0.9429\n",
      "Epoch 2. Loss: 0.18661645533243815, Train_acc 0.95385, Test_acc 0.9593\n",
      "Epoch 3. Loss: 0.14246525653998057, Train_acc 0.9601333333333333, Test_acc 0.963\n",
      "Epoch 4. Loss: 0.11766305784781773, Train_acc 0.9695166666666667, Test_acc 0.9723\n",
      "Epoch 5. Loss: 0.10152678947846094, Train_acc 0.97245, Test_acc 0.9763\n",
      "Epoch 6. Loss: 0.08953877588311832, Train_acc 0.9737833333333333, Test_acc 0.9777\n",
      "Epoch 7. Loss: 0.07995961449742317, Train_acc 0.9780833333333333, Test_acc 0.9797\n",
      "Epoch 8. Loss: 0.07317343327899774, Train_acc 0.9795833333333334, Test_acc 0.9811\n",
      "Epoch 9. Loss: 0.06775576130946477, Train_acc 0.9798, Test_acc 0.981\n"
     ]
    }
   ],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    \"\"\"Make predictions for the dataset and evaluate average accuracy.\"\"\"\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        # ==== note the difference in raw data input shape ====\n",
    "        # use 4d tensor (batch_size, 1, 28, 28)\n",
    "        data = data.as_in_context(model_ctx)\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1]\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': .01})\n",
    "\n",
    "epochs = 10\n",
    "smoothing_constant = .01\n",
    "num_examples = 60000\n",
    "\n",
    "for e in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    for i, (data, label) in enumerate(train_iter):\n",
    "        # ==== note the difference in raw data input shape ====\n",
    "        # use 4d tensor (batch_size, 1, 28, 28)\n",
    "        data = data.as_in_context(model_ctx)\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        cumulative_loss += nd.sum(loss).asscalar()\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_iter, net)\n",
    "    train_accuracy = evaluate_accuracy(train_iter, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" %\n",
    "          (e, cumulative_loss/num_examples, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ekTAkGy0FrCb"
   },
   "source": [
    "You should see that ConvNets achieve a higher accuracy than MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "6QcNrzAQ9uMw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "lab1-mnist.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
